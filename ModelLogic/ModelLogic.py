import ollama
import chromadb
import psycopg
import ast
from tqdm import tqdm
from psycopg.rows import dict_row
from colorama import Fore

client = chromadb.Client()
chatModel = "llama3.1"
encodingModel = "nomic-embed-text:latest"

def set_chat_model(model: str):
    global chatModel
    chatModel = model
    print("New Chat Model: " + model)

def set_encoding_model(model: str):
    global encodingModel
    encodingModel = model
    print("New encoding Model: " + model)


system_prompt = (
'You are an AI assistant that has memory of every conversation you have ever had with the user.'
'On every prompt from the user, the system has checked for any relevant messages you have had with the user.'
'If any embedded previous conversations are attached, use them for context to responding to the user,'
'if the context is relevant and useful to responding. If the realled conversation is irrelevant,'
'disregard speaking about them and respond normally as an AI assistant. Do not talk about recalling conversations.'
'Just use any useful data from the previous conversations and respond normally as an intelligent AI assistant.'
)

convo = [{'role': 'system', 'content': system_prompt}]


DB_PARAMS = {
    'dbname': 'memory_agent',
    'user': 'lfinch',
    'host': '/run/postgresql',
}

def list_models():
    result = ollama.list()
    return [model["model"] for model in result["models"]]


def connect_db():
    conn = psycopg.connect(**DB_PARAMS)
    return conn

def fetch_conversations():
    conn = connect_db()
    with conn.cursor(row_factory=dict_row) as cursor:
        cursor.execute('SELECT * FROM conversations')
        conversations = cursor.fetchall()
    conn.close()
    return conversations

def store_conversations(prompt, response):
    conn = connect_db()
    with conn.cursor() as cursor:
        cursor.execute(
            'INSERT INTO conversations (timestamp, prompt, response) VALUES (CURRENT_TIMESTAMP, %s, %s)', (prompt, response)
        )
        conn.commit()
    conn.close()

def remove_last_conversation():#this is broken
    conn = connect_db()
    with conn.cursor() as cursor:
        cursor.execute('DELETE FROM conversations WHERE id = (SELECT MAX(id) FROM conversations)')
        cursor.commit()
    conn.close()

def stream_response(prompt):
    response = ''
    stream = ollama.chat(model=chatModel, messages=convo, stream=True)
    print(Fore.LIGHTGREEN_EX + '\nASSISTANT:')

    for chunk in stream:
        content = chunk['message']['content']
        response += content
        print(content, end='', flush=True)

    print('\n')
    store_conversations(prompt=prompt, response=response)
    convo.append({'role': 'assistant', 'content': response})

def standard_response(prompt):
    response = ollama.chat(model=chatModel, messages=convo)
    responseString = response["message"]["content"]
    print(Fore.LIGHTGREEN_EX + '\nASSISTANT: \n ' + responseString)
    store_conversations(prompt=prompt, response=responseString)
    convo.append({'role': 'assistant', 'content': responseString})
    return responseString

def create_vector_db(conversations):
    vector_db_name = 'conversations'
    existing = [c.name for c in client.list_collections()]

    if vector_db_name in existing:
        client.delete_collection(name=vector_db_name)


    vector_db = client.create_collection(name=vector_db_name)

    for c in conversations:
        serialized_convo = f'prompt: {c['prompt']} response: {c['response']}'
        response = ollama.embeddings(model=encodingModel, prompt=serialized_convo)
        embedding = response['embedding']

        vector_db.add(
            ids=[str(c['id'])],
            embeddings=[embedding],
            documents=[serialized_convo],
        )

def retrieve_embeddings(queries, results_per_query=2):
    embeddings = set()

    for query in tqdm(queries, desc='Processing queries to vector database'):
        response = ollama.embeddings(model=encodingModel, prompt=query)
        query_embedding = response['embedding']

        vector_db = client.get_collection(name='conversations')
        results = vector_db.query(query_embeddings=[query_embedding], n_results=results_per_query)
        best_embeddings = results['documents'][0]

        for best in best_embeddings:
            if best not in embeddings:
                if 'yes' in classify_embedding(query=query, context=best):
                    embeddings.add(best)

    return embeddings


def create_queries(prompt):
    query_msg = (
        'You are a first principal reasoning search query AI agent. '
        'Your list of search queries will be ran on an embedding database of all your conversations '
        'you have ever had with the user. With first principals create a python list of queries to '
        'search the embeddings database for any data that would be necessary to have access to in '
        'order to correctly respond to the prompt. Your response must be a Python list with no syntax errors. '
        'Do not explain anything and do not ever generate anything but a perfect syntax Python list'
    )
    query_convo = [
        {'role': 'system', 'content': query_msg},
        {'role': 'user', 'content': 'Write an email to my car insurance company and create a pursuasive request for them to lower my monthly rate.'},
        {'role': 'assistant', 'content': '["What is the users name?", "What is the users current auto insurance provider?", "What is the monthly rate the user currently pays for auto insurance?"]'},
        {'role': 'user', 'content': 'how can I convert the speak function in my llama3 python voice assistant to use pyttsx3 instead of OpanAI TTS?'},
        {'role': 'assistant', 'content': '["Llama3 voice assistant", "Python voice assistant", "OpanAI TTS", "openai speak"]'},
        {'role': 'user', 'content': prompt}
    ]

    response = ollama.chat(model=chatModel, messages=query_convo)
    print(Fore.YELLOW + f'\nVector database queries: {response["message"]["content"]}\n')

    try:
        return ast.literal_eval(response['message']['content'])
    except:
        return [prompt]
    
def classify_embedding(query, context):
    classify_msg = (
        'You are an embedding classification AI agent. Your input will be a prompt and one embedded chunk of text. '
        'You will not respond as an AI assistant. You only respond "yes" or "no". '
        'Determine whether the context contains data that directly is related to the search query. '
        'If the context is seemingly exactly what the search query needs, respond "yes" if it is anything but directly '
        'related respond "no". Do not respond "yes" unless the content is highly relevant to the search query.'
    )
    classify_convo = [
        {'role': 'system', 'content': classify_msg},
        {'role': 'user', 'content': f'SEARCH QUERY: What is the users name? \n\nEMBEDDED CONTENT: You are Luke Finch. How can I help today Luke?'},
        {'role': 'assistant', 'content': 'yes'},
        {'role': 'user', 'content': f'SEARCH QUERYL Llama 3 Python Voice Assistant \n\nEMBEDDED CONTENT: Siri is a voice assistant on Apple iOS and Mac OS.'},
        {'role': 'assistant', 'content': 'no'},
        {'role': 'user', 'content': f'SEARCH QUERY: {query} \n\nEMBEDDED CONTEXT: {context}'}
    ]

    response = ollama.chat(model=chatModel, messages=classify_convo)

    return response['message']['content'].strip().lower()

def recall(prompt):
    queries = create_queries(prompt=prompt)
    embeddings = retrieve_embeddings(queries=queries)
    convo.append({'role': 'user', 'content': f'MEMORIES: {embeddings} \n\n USER PROMPT: {prompt}'})
    print(f'\n{len(embeddings)} message: response embeddings added for context.')

def search(prompt):
    print("searching")

conversations = fetch_conversations()
create_vector_db(conversations=conversations)

def handle_prompt(prompt: str) -> str:
    global convo

    clean_prompt = prompt.strip()

    if clean_prompt.lower().startswith("/recall"):
        clean_prompt = clean_prompt[8:].strip()
        recall(prompt=clean_prompt)
        response = stream_response(prompt=clean_prompt)
        return response

    elif clean_prompt.lower().startswith("/search"):
        clean_prompt = clean_prompt[8:].strip()
        search(clean_prompt)
        response = stream_response(prompt=clean_prompt)
        return response

    elif clean_prompt.lower().startswith("/forget"):
        remove_last_conversation()
        convo = convo[:-2]
        return "Forgotten by Model"
        
    elif clean_prompt.lower().startswith("/memorize"):
        clean_prompt = clean_prompt[10:].strip()
        store_conversations(prompt=clean_prompt, response='Memory stored. ')
        return "Memory stored. "

    else:
        convo.append({'role': 'user', 'content': clean_prompt})
        response = standard_response(prompt=prompt)
        return response